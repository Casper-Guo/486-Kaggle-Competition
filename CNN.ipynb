{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15744405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7905a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = df_train[\"text\"].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned)\n",
    "sequences = tokenizer.texts_to_sequences(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max([len(s.split()) for s in cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1699d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac3bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length)) \n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(sequences, df_train[\"label\"], validation_split=0.2, epochs=5, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = hist.history['accuracy']\n",
    "val = hist.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
    "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test = df_test[\"text\"].apply(remove_stop_words)\n",
    "sequence_test = tokenizer.texts_to_sequences(cleaned_test)\n",
    "sequence_test = pad_sequences(sequence_test, maxlen=max_length)\n",
    "pred = model.predict(sequence_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06128c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Category\"] = df_test.apply(lambda row: 0 if pred[row.name][0] < 0.5 else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[[\"Id\", \"Category\"]].to_csv(\"CNN.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
